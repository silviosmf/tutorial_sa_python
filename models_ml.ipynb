{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorer Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load product reviews preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_score</th>\n",
       "      <th>review_comment_message</th>\n",
       "      <th>review_creation_date</th>\n",
       "      <th>label</th>\n",
       "      <th>processed_review_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Só achei ela pequena pra seis xícaras ,mais é ...</td>\n",
       "      <td>2017-08-08 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>achar pequeno pra seis xícara , bom produto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Entrega antes da data marcada. Excelente</td>\n",
       "      <td>2018-06-20 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>entregar antes data marcar . excelente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>estou satisfeito</td>\n",
       "      <td>2018-08-15 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>satisfeito</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Mais uma ve satisfeito</td>\n",
       "      <td>2018-05-09 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>ve satisfeito</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Muito boa a compra, dentro do prazo.</td>\n",
       "      <td>2017-12-08 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>bom compra , dentro prazo .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_score                             review_comment_message  \\\n",
       "0             5  Só achei ela pequena pra seis xícaras ,mais é ...   \n",
       "1             5           Entrega antes da data marcada. Excelente   \n",
       "2             5                                   estou satisfeito   \n",
       "3             5                            Mais uma ve satisfeito    \n",
       "4             5               Muito boa a compra, dentro do prazo.   \n",
       "\n",
       "  review_creation_date  label                     processed_review_comment  \n",
       "0  2017-08-08 00:00:00      1  achar pequeno pra seis xícara , bom produto  \n",
       "1  2018-06-20 00:00:00      1       entregar antes data marcar . excelente  \n",
       "2  2018-08-15 00:00:00      1                                   satisfeito  \n",
       "3  2018-05-09 00:00:00      1                                ve satisfeito  \n",
       "4  2017-12-08 00:00:00      1                  bom compra , dentro prazo .  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews = pd.read_csv('./data/product_reviews_preprocessed.csv')\n",
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW, TF-IDF and Word2Vec\n",
    "\n",
    "Functions that return values from textual representations in BoW, TF-IDF and Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_representation(df):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(df['processed_review_comment'])\n",
    "    y = df['label']\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def tfidf_representation(df):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(df['processed_review_comment'])\n",
    "    y = df['label']\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def word2vec_representation(df, model_path):\n",
    "    model = KeyedVectors.load_word2vec_format(model_path, unicode_errors='ignore')\n",
    "\n",
    "    def get_vector(text):\n",
    "        words = text.split()\n",
    "        word_vectors = [model[word] for word in words if word in model]\n",
    "        if len(word_vectors) == 0:\n",
    "            return np.zeros(50)\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "\n",
    "    X = df['processed_review_comment'].apply(get_vector)\n",
    "    y = df['label']\n",
    "    return train_test_split(np.vstack(X.values), y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Machine Learning \n",
    "Logistic Regression, Naive Bayes and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: Logistic Regression - Representação: Bag-of-Words\n",
      "Acurácia: 0.9526074700493306\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92      1644\n",
      "           1       0.97      0.96      0.97      4032\n",
      "\n",
      "    accuracy                           0.95      5676\n",
      "   macro avg       0.94      0.95      0.94      5676\n",
      "weighted avg       0.95      0.95      0.95      5676\n",
      "\n",
      "\n",
      "Modelo: Logistic Regression - Representação: TF-IDF\n",
      "Acurácia: 0.9506694855532065\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92      1644\n",
      "           1       0.98      0.95      0.96      4032\n",
      "\n",
      "    accuracy                           0.95      5676\n",
      "   macro avg       0.93      0.95      0.94      5676\n",
      "weighted avg       0.95      0.95      0.95      5676\n",
      "\n",
      "\n",
      "Modelo: Logistic Regression - Representação: Word2Vec\n",
      "Acurácia: 0.8537702607470049\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.88      0.78      1644\n",
      "           1       0.95      0.84      0.89      4032\n",
      "\n",
      "    accuracy                           0.85      5676\n",
      "   macro avg       0.82      0.86      0.83      5676\n",
      "weighted avg       0.87      0.85      0.86      5676\n",
      "\n",
      "\n",
      "Modelo: Naive Bayes - Representação: Bag-of-Words\n",
      "Acurácia: 0.9459126145172657\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.92      0.91      1644\n",
      "           1       0.97      0.96      0.96      4032\n",
      "\n",
      "    accuracy                           0.95      5676\n",
      "   macro avg       0.93      0.94      0.93      5676\n",
      "weighted avg       0.95      0.95      0.95      5676\n",
      "\n",
      "\n",
      "Modelo: Naive Bayes - Representação: TF-IDF\n",
      "Acurácia: 0.9367512332628611\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.86      0.89      1644\n",
      "           1       0.95      0.97      0.96      4032\n",
      "\n",
      "    accuracy                           0.94      5676\n",
      "   macro avg       0.93      0.92      0.92      5676\n",
      "weighted avg       0.94      0.94      0.94      5676\n",
      "\n",
      "\n",
      "Modelo: SVM - Representação: Bag-of-Words\n",
      "Acurácia: 0.9464411557434813\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91      1644\n",
      "           1       0.98      0.95      0.96      4032\n",
      "\n",
      "    accuracy                           0.95      5676\n",
      "   macro avg       0.93      0.95      0.94      5676\n",
      "weighted avg       0.95      0.95      0.95      5676\n",
      "\n",
      "\n",
      "Modelo: SVM - Representação: TF-IDF\n",
      "Acurácia: 0.95630725863284\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93      1644\n",
      "           1       0.98      0.96      0.97      4032\n",
      "\n",
      "    accuracy                           0.96      5676\n",
      "   macro avg       0.94      0.95      0.95      5676\n",
      "weighted avg       0.96      0.96      0.96      5676\n",
      "\n",
      "\n",
      "Modelo: SVM - Representação: Word2Vec\n",
      "Acurácia: 0.8994009866102889\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.92      0.84      1644\n",
      "           1       0.96      0.89      0.93      4032\n",
      "\n",
      "    accuracy                           0.90      5676\n",
      "   macro avg       0.87      0.91      0.88      5676\n",
      "weighted avg       0.91      0.90      0.90      5676\n",
      "\n",
      "\n",
      "Modelo: Random Forest - Representação: Bag-of-Words\n",
      "Acurácia: 0.9478505990133897\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91      1644\n",
      "           1       0.96      0.96      0.96      4032\n",
      "\n",
      "    accuracy                           0.95      5676\n",
      "   macro avg       0.94      0.94      0.94      5676\n",
      "weighted avg       0.95      0.95      0.95      5676\n",
      "\n",
      "\n",
      "Modelo: Random Forest - Representação: TF-IDF\n",
      "Acurácia: 0.9464411557434813\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.91      0.91      1644\n",
      "           1       0.97      0.96      0.96      4032\n",
      "\n",
      "    accuracy                           0.95      5676\n",
      "   macro avg       0.93      0.94      0.94      5676\n",
      "weighted avg       0.95      0.95      0.95      5676\n",
      "\n",
      "\n",
      "Modelo: Random Forest - Representação: Word2Vec\n",
      "Acurácia: 0.8736786469344608\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.70      0.76      1644\n",
      "           1       0.89      0.94      0.91      4032\n",
      "\n",
      "    accuracy                           0.87      5676\n",
      "   macro avg       0.86      0.82      0.84      5676\n",
      "weighted avg       0.87      0.87      0.87      5676\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate(model, X_train, X_test, y_train, y_test, model_name, representation_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print(f\"Modelo: {model_name} - Representação: {representation_name}\\nAcurácia: {accuracy}\\nRelatório de Classificação:\\n{report}\\n\")\n",
    "\n",
    "# Modelos a serem usados\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(class_weight='balanced'),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'SVM': SVC(class_weight='balanced'),\n",
    "    'Random Forest': RandomForestClassifier(class_weight='balanced')\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    # Bag-of-Words\n",
    "    X_train, X_test, y_train, y_test = bow_representation(df_reviews)\n",
    "    train_and_evaluate(model, X_train, X_test, y_train, y_test, model_name, \"Bag-of-Words\")\n",
    "\n",
    "    # TF-IDF\n",
    "    X_train, X_test, y_train, y_test = tfidf_representation(df_reviews)\n",
    "    train_and_evaluate(model, X_train, X_test, y_train, y_test, model_name, \"TF-IDF\")\n",
    "\n",
    "    # Word2Vec (excluindo Naive Bayes)\n",
    "    if model_name != 'Naive Bayes':\n",
    "        X_train, X_test, y_train, y_test = word2vec_representation(df_reviews, './data/cbow_s50.txt')\n",
    "        train_and_evaluate(model, X_train, X_test, y_train, y_test, model_name, \"Word2Vec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: Logistic Regression - Representação: Bag-of-Words\n",
      "Acurácia: 0.9526074700493306\n",
      "Relatório de Classificação:\n",
      "{'0': {'precision': 0.9109384339509863, 'recall': 0.927007299270073, 'f1-score': 0.9189026228519745, 'support': 1644.0}, '1': {'precision': 0.9700224831376467, 'recall': 0.9630456349206349, 'f1-score': 0.9665214685749844, 'support': 4032.0}, 'accuracy': 0.9526074700493306, 'macro avg': {'precision': 0.9404804585443165, 'recall': 0.9450264670953539, 'f1-score': 0.9427120457134794, 'support': 5676.0}, 'weighted avg': {'precision': 0.9529093441554639, 'recall': 0.9526074700493306, 'f1-score': 0.952729117911026, 'support': 5676.0}}\n",
      "\n",
      "Modelo: Logistic Regression - Representação: TF-IDF\n",
      "Acurácia: 0.9506694855532065\n",
      "Relatório de Classificação:\n",
      "{'0': {'precision': 0.8870601589103292, 'recall': 0.9507299270072993, 'f1-score': 0.9177921315325894, 'support': 1644.0}, '1': {'precision': 0.9793050587634133, 'recall': 0.9506448412698413, 'f1-score': 0.9647621444752076, 'support': 4032.0}, 'accuracy': 0.9506694855532065, 'macro avg': {'precision': 0.9331826088368713, 'recall': 0.9506873841385703, 'f1-score': 0.9412771380038985, 'support': 5676.0}, 'weighted avg': {'precision': 0.9525871913641055, 'recall': 0.9506694855532065, 'f1-score': 0.9511577221218488, 'support': 5676.0}}\n",
      "\n",
      "Modelo: Logistic Regression - Representação: Word2Vec\n",
      "Acurácia: 0.8537702607470049\n",
      "Relatório de Classificação:\n",
      "{'0': {'precision': 0.6947368421052632, 'recall': 0.8832116788321168, 'f1-score': 0.7777182645956079, 'support': 1644.0}, '1': {'precision': 0.9464584495259342, 'recall': 0.841765873015873, 'f1-score': 0.8910475190338673, 'support': 4032.0}, 'accuracy': 0.8537702607470049, 'macro avg': {'precision': 0.8205976458155987, 'recall': 0.862488775923995, 'f1-score': 0.8343828918147376, 'support': 5676.0}, 'weighted avg': {'precision': 0.8735496541419343, 'recall': 0.8537702607470049, 'f1-score': 0.8582227666912848, 'support': 5676.0}}\n",
      "\n",
      "Modelo: Naive Bayes - Representação: Bag-of-Words\n",
      "Acurácia: 0.9459126145172657\n",
      "Relatório de Classificação:\n",
      "{'0': {'precision': 0.8981536628945801, 'recall': 0.9172749391727494, 'f1-score': 0.9076136021667167, 'support': 1644.0}, '1': {'precision': 0.9659744808606455, 'recall': 0.9575892857142857, 'f1-score': 0.9617636069248973, 'support': 4032.0}, 'accuracy': 0.9459126145172657, 'macro avg': {'precision': 0.9320640718776128, 'recall': 0.9374321124435175, 'f1-score': 0.9346886045458069, 'support': 5676.0}, 'weighted avg': {'precision': 0.9463308189973242, 'recall': 0.9459126145172657, 'f1-score': 0.9460795674917668, 'support': 5676.0}}\n",
      "\n",
      "Modelo: Naive Bayes - Representação: TF-IDF\n",
      "Acurácia: 0.9367512332628611\n",
      "Relatório de Classificação:\n",
      "{'0': {'precision': 0.9121231558691469, 'recall': 0.864963503649635, 'f1-score': 0.8879175772713082, 'support': 1644.0}, '1': {'precision': 0.9460772407092544, 'recall': 0.9660218253968254, 'f1-score': 0.9559455147870904, 'support': 4032.0}, 'accuracy': 0.9367512332628611, 'macro avg': {'precision': 0.9291001982892007, 'recall': 0.9154926645232302, 'f1-score': 0.9219315460291992, 'support': 5676.0}, 'weighted avg': {'precision': 0.9362427594764959, 'recall': 0.9367512332628611, 'f1-score': 0.9362418626947815, 'support': 5676.0}}\n",
      "\n",
      "Modelo: SVM - Representação: Bag-of-Words\n",
      "Acurácia: 0.9464411557434813\n",
      "Relatório de Classificação:\n",
      "{'0': {'precision': 0.8811149032992036, 'recall': 0.9422141119221411, 'f1-score': 0.9106407995296883, 'support': 1644.0}, '1': {'precision': 0.9757529351710056, 'recall': 0.9481646825396826, 'f1-score': 0.961761006289308, 'support': 4032.0}, 'accuracy': 0.9464411557434813, 'macro avg': {'precision': 0.9284339192351045, 'recall': 0.9451893972309118, 'f1-score': 0.9362009029094982, 'support': 5676.0}, 'weighted avg': {'precision': 0.9483419195971432, 'recall': 0.9464411557434813, 'f1-score': 0.9469545193420187, 'support': 5676.0}}\n",
      "\n",
      "Modelo: SVM - Representação: TF-IDF\n",
      "Acurácia: 0.95630725863284\n",
      "Relatório de Classificação:\n",
      "{'0': {'precision': 0.9077102803738317, 'recall': 0.9452554744525548, 'f1-score': 0.9261025029797377, 'support': 1644.0}, '1': {'precision': 0.9772956609485368, 'recall': 0.9608134920634921, 'f1-score': 0.9689844922461232, 'support': 4032.0}, 'accuracy': 0.95630725863284, 'macro avg': {'precision': 0.9425029706611843, 'recall': 0.9530344832580234, 'f1-score': 0.9475434976129304, 'support': 5676.0}, 'weighted avg': {'precision': 0.9571409101266878, 'recall': 0.95630725863284, 'f1-score': 0.9565641274903202, 'support': 5676.0}}\n",
      "\n",
      "Modelo: SVM - Representação: Word2Vec\n",
      "Acurácia: 0.8994009866102889\n",
      "Relatório de Classificação:\n",
      "{'0': {'precision': 0.7755521314843349, 'recall': 0.9184914841849149, 'f1-score': 0.8409913673071567, 'support': 1644.0}, '1': {'precision': 0.9640654330919818, 'recall': 0.8916170634920635, 'f1-score': 0.9264270068290169, 'support': 4032.0}, 'accuracy': 0.8994009866102889, 'macro avg': {'precision': 0.8698087822881584, 'recall': 0.9050542738384892, 'f1-score': 0.8837091870680869, 'support': 5676.0}, 'weighted avg': {'precision': 0.9094643288208452, 'recall': 0.8994009866102889, 'f1-score': 0.9016813776228968, 'support': 5676.0}}\n",
      "\n",
      "Modelo: Random Forest - Representação: Bag-of-Words\n",
      "Acurácia: 0.9471458773784355\n",
      "Relatório de Classificação:\n",
      "{'0': {'precision': 0.9092570036540804, 'recall': 0.9081508515815085, 'f1-score': 0.9087035909920876, 'support': 1644.0}, '1': {'precision': 0.9625681705503223, 'recall': 0.9630456349206349, 'f1-score': 0.9628068435407885, 'support': 4032.0}, 'accuracy': 0.9471458773784355, 'macro avg': {'precision': 0.9359125871022014, 'recall': 0.9355982432510717, 'f1-score': 0.9357552172664381, 'support': 5676.0}, 'weighted avg': {'precision': 0.9471270926120874, 'recall': 0.9471458773784355, 'f1-score': 0.9471363454452874, 'support': 5676.0}}\n",
      "\n",
      "Modelo: Random Forest - Representação: TF-IDF\n",
      "Acurácia: 0.9464411557434813\n",
      "Relatório de Classificação:\n",
      "{'0': {'precision': 0.9016786570743405, 'recall': 0.9148418491484185, 'f1-score': 0.9082125603864734, 'support': 1644.0}, '1': {'precision': 0.9650698602794411, 'recall': 0.9593253968253969, 'f1-score': 0.9621890547263681, 'support': 4032.0}, 'accuracy': 0.9464411557434813, 'macro avg': {'precision': 0.9333742586768907, 'recall': 0.9370836229869077, 'f1-score': 0.9352008075564207, 'support': 5676.0}, 'weighted avg': {'precision': 0.9467091946576678, 'recall': 0.9464411557434813, 'f1-score': 0.9465552709535021, 'support': 5676.0}}\n",
      "\n",
      "Modelo: Random Forest - Representação: Word2Vec\n",
      "Acurácia: 0.8747357293868921\n",
      "Relatório de Classificação:\n",
      "{'0': {'precision': 0.845299777942265, 'recall': 0.694647201946472, 'f1-score': 0.7626043405676127, 'support': 1644.0}, '1': {'precision': 0.8839306358381502, 'recall': 0.9481646825396826, 'f1-score': 0.9149216225918392, 'support': 4032.0}, 'accuracy': 0.8747357293868921, 'macro avg': {'precision': 0.8646152068902075, 'recall': 0.8214059422430773, 'f1-score': 0.838762981579726, 'support': 5676.0}, 'weighted avg': {'precision': 0.872741571289025, 'recall': 0.8747357293868921, 'f1-score': 0.8708043548596637, 'support': 5676.0}}\n",
      "\n",
      "Modelo: Logistic Regression - Representação: Bag-of-Words\n",
      "Acurácia: 0.9526074700493306\n",
      "Relatório de Classificação:\n",
      "{'0': {'precision': 0.9109384339509863, 'recall': 0.927007299270073, 'f1-score': 0.9189026228519745, 'support': 1644.0}, '1': {'precision': 0.9700224831376467, 'recall': 0.9630456349206349, 'f1-score': 0.9665214685749844, 'support': 4032.0}, 'accuracy': 0.9526074700493306, 'macro avg': {'precision': 0.9404804585443165, 'recall': 0.9450264670953539, 'f1-score': 0.9427120457134794, 'support': 5676.0}, 'weighted avg': {'precision': 0.9529093441554639, 'recall': 0.9526074700493306, 'f1-score': 0.952729117911026, 'support': 5676.0}}\n",
      "\n",
      "Modelo: Naive Bayes - Representação: Bag-of-Words\n",
      "Acurácia: 0.9459126145172657\n",
      "Relatório de Classificação:\n",
      "{'0': {'precision': 0.8981536628945801, 'recall': 0.9172749391727494, 'f1-score': 0.9076136021667167, 'support': 1644.0}, '1': {'precision': 0.9659744808606455, 'recall': 0.9575892857142857, 'f1-score': 0.9617636069248973, 'support': 4032.0}, 'accuracy': 0.9459126145172657, 'macro avg': {'precision': 0.9320640718776128, 'recall': 0.9374321124435175, 'f1-score': 0.9346886045458069, 'support': 5676.0}, 'weighted avg': {'precision': 0.9463308189973242, 'recall': 0.9459126145172657, 'f1-score': 0.9460795674917668, 'support': 5676.0}}\n",
      "\n",
      "Modelo: SVM - Representação: Bag-of-Words\n",
      "Acurácia: 0.9464411557434813\n",
      "Relatório de Classificação:\n",
      "{'0': {'precision': 0.8811149032992036, 'recall': 0.9422141119221411, 'f1-score': 0.9106407995296883, 'support': 1644.0}, '1': {'precision': 0.9757529351710056, 'recall': 0.9481646825396826, 'f1-score': 0.961761006289308, 'support': 4032.0}, 'accuracy': 0.9464411557434813, 'macro avg': {'precision': 0.9284339192351045, 'recall': 0.9451893972309118, 'f1-score': 0.9362009029094982, 'support': 5676.0}, 'weighted avg': {'precision': 0.9483419195971432, 'recall': 0.9464411557434813, 'f1-score': 0.9469545193420187, 'support': 5676.0}}\n",
      "\n",
      "Modelo: Random Forest - Representação: Bag-of-Words\n",
      "Acurácia: 0.9455602536997886\n",
      "Relatório de Classificação:\n",
      "{'0': {'precision': 0.9097605893186004, 'recall': 0.9014598540145985, 'f1-score': 0.9055912007332723, 'support': 1644.0}, '1': {'precision': 0.9599703484062269, 'recall': 0.9635416666666666, 'f1-score': 0.9617526921648718, 'support': 4032.0}, 'accuracy': 0.9455602536997886, 'macro avg': {'precision': 0.9348654688624136, 'recall': 0.9325007603406326, 'f1-score': 0.933671946449072, 'support': 5676.0}, 'weighted avg': {'precision': 0.9454275640616078, 'recall': 0.9455602536997886, 'f1-score': 0.94548604454092, 'support': 5676.0}}\n",
      "\n",
      "                Modelo Representação  Acurácia  Precision 0  Recall 0  \\\n",
      "0  Logistic Regression  Bag-of-Words  0.952607     0.910938  0.927007   \n",
      "1          Naive Bayes  Bag-of-Words  0.945913     0.898154  0.917275   \n",
      "2                  SVM  Bag-of-Words  0.946441     0.881115  0.942214   \n",
      "3        Random Forest  Bag-of-Words  0.945560     0.909761  0.901460   \n",
      "\n",
      "   F1-Score 0  Precision 1  Recall 1  F1-Score 1  \n",
      "0    0.918903     0.970022  0.963046    0.966521  \n",
      "1    0.907614     0.965974  0.957589    0.961764  \n",
      "2    0.910641     0.975753  0.948165    0.961761  \n",
      "3    0.905591     0.959970  0.963542    0.961753  \n"
     ]
    }
   ],
   "source": [
    "# Modelos a serem usados\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(class_weight='balanced'),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'SVM': SVC(class_weight='balanced'),\n",
    "    'Random Forest': RandomForestClassifier(class_weight='balanced')\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    # Bag-of-Words\n",
    "    X_train, X_test, y_train, y_test = bow_representation(df_reviews)\n",
    "    train_and_evaluate(model, X_train, X_test, y_train, y_test, model_name, \"Bag-of-Words\")\n",
    "\n",
    "    # TF-IDF\n",
    "    X_train, X_test, y_train, y_test = tfidf_representation(df_reviews)\n",
    "    train_and_evaluate(model, X_train, X_test, y_train, y_test, model_name, \"TF-IDF\")\n",
    "\n",
    "    # Word2Vec (excluindo Naive Bayes)\n",
    "    if model_name != 'Naive Bayes':\n",
    "        X_train, X_test, y_train, y_test = word2vec_representation(df_reviews, './data/cbow_s50.txt')\n",
    "        train_and_evaluate(model, X_train, X_test, y_train, y_test, model_name, \"Word2Vec\")\n",
    "\n",
    "def train_and_evaluate(model, X_train, X_test, y_train, y_test, model_name, representation_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "    print(f\"Modelo: {model_name} - Representação: {representation_name}\\nAcurácia: {accuracy}\\nRelatório de Classificação:\\n{report_dict}\\n\")\n",
    "    \n",
    "    # Armazenando as métricas em um dicionário\n",
    "    metrics = {\n",
    "        'Modelo': model_name,\n",
    "        'Representação': representation_name,\n",
    "        'Acurácia': accuracy,\n",
    "        'Precision 0': report_dict['0']['precision'],\n",
    "        'Recall 0': report_dict['0']['recall'],\n",
    "        'F1-Score 0': report_dict['0']['f1-score'],\n",
    "        'Precision 1': report_dict['1']['precision'],\n",
    "        'Recall 1': report_dict['1']['recall'],\n",
    "        'F1-Score 1': report_dict['1']['f1-score']\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# Lista para armazenar os resultados\n",
    "results = []\n",
    "\n",
    "\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    # Repita para cada representação como Bag-of-Words, TF-IDF, Word2Vec\n",
    "    # Exemplo para Bag-of-Words\n",
    "    X_train, X_test, y_train, y_test = bow_representation(df_reviews)\n",
    "    result = train_and_evaluate(model, X_train, X_test, y_train, y_test, model_name, \"Bag-of-Words\")\n",
    "\n",
    "    # TF-IDF\n",
    "    X_train, X_test, y_train, y_test = tfidf_representation(df_reviews)\n",
    "    result = train_and_evaluate(model, X_train, X_test, y_train, y_test, model_name, \"TF-IDF\")\n",
    "\n",
    "    # Word2Vec (excluindo Naive Bayes)\n",
    "    if model_name != 'Naive Bayes':\n",
    "        X_train, X_test, y_train, y_test = word2vec_representation(df_reviews, './data/cbow_s50.txt')\n",
    "        result = train_and_evaluate(model, X_train, X_test, y_train, y_test, model_name, \"Word2Vec\")\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "# Convertendo a lista de resultados em um DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression - Representation: Bag-of-Words\n",
      "Accuracy: 0.9526074700493306\n",
      "Classification Report:\n",
      "{'0': {'precision': 0.9109384339509863, 'recall': 0.927007299270073, 'f1-score': 0.9189026228519745, 'support': 1644.0}, '1': {'precision': 0.9700224831376467, 'recall': 0.9630456349206349, 'f1-score': 0.9665214685749844, 'support': 4032.0}, 'accuracy': 0.9526074700493306, 'macro avg': {'precision': 0.9404804585443165, 'recall': 0.9450264670953539, 'f1-score': 0.9427120457134794, 'support': 5676.0}, 'weighted avg': {'precision': 0.9529093441554639, 'recall': 0.9526074700493306, 'f1-score': 0.952729117911026, 'support': 5676.0}}\n",
      "\n",
      "Model: Logistic Regression - Representation: TF-IDF\n",
      "Accuracy: 0.9506694855532065\n",
      "Classification Report:\n",
      "{'0': {'precision': 0.8870601589103292, 'recall': 0.9507299270072993, 'f1-score': 0.9177921315325894, 'support': 1644.0}, '1': {'precision': 0.9793050587634133, 'recall': 0.9506448412698413, 'f1-score': 0.9647621444752076, 'support': 4032.0}, 'accuracy': 0.9506694855532065, 'macro avg': {'precision': 0.9331826088368713, 'recall': 0.9506873841385703, 'f1-score': 0.9412771380038985, 'support': 5676.0}, 'weighted avg': {'precision': 0.9525871913641055, 'recall': 0.9506694855532065, 'f1-score': 0.9511577221218488, 'support': 5676.0}}\n",
      "\n",
      "Model: Logistic Regression - Representation: Word2Vec\n",
      "Accuracy: 0.8537702607470049\n",
      "Classification Report:\n",
      "{'0': {'precision': 0.6947368421052632, 'recall': 0.8832116788321168, 'f1-score': 0.7777182645956079, 'support': 1644.0}, '1': {'precision': 0.9464584495259342, 'recall': 0.841765873015873, 'f1-score': 0.8910475190338673, 'support': 4032.0}, 'accuracy': 0.8537702607470049, 'macro avg': {'precision': 0.8205976458155987, 'recall': 0.862488775923995, 'f1-score': 0.8343828918147376, 'support': 5676.0}, 'weighted avg': {'precision': 0.8735496541419343, 'recall': 0.8537702607470049, 'f1-score': 0.8582227666912848, 'support': 5676.0}}\n",
      "\n",
      "Model: Naive Bayes - Representation: Bag-of-Words\n",
      "Accuracy: 0.9459126145172657\n",
      "Classification Report:\n",
      "{'0': {'precision': 0.8981536628945801, 'recall': 0.9172749391727494, 'f1-score': 0.9076136021667167, 'support': 1644.0}, '1': {'precision': 0.9659744808606455, 'recall': 0.9575892857142857, 'f1-score': 0.9617636069248973, 'support': 4032.0}, 'accuracy': 0.9459126145172657, 'macro avg': {'precision': 0.9320640718776128, 'recall': 0.9374321124435175, 'f1-score': 0.9346886045458069, 'support': 5676.0}, 'weighted avg': {'precision': 0.9463308189973242, 'recall': 0.9459126145172657, 'f1-score': 0.9460795674917668, 'support': 5676.0}}\n",
      "\n",
      "Model: Naive Bayes - Representation: TF-IDF\n",
      "Accuracy: 0.9367512332628611\n",
      "Classification Report:\n",
      "{'0': {'precision': 0.9121231558691469, 'recall': 0.864963503649635, 'f1-score': 0.8879175772713082, 'support': 1644.0}, '1': {'precision': 0.9460772407092544, 'recall': 0.9660218253968254, 'f1-score': 0.9559455147870904, 'support': 4032.0}, 'accuracy': 0.9367512332628611, 'macro avg': {'precision': 0.9291001982892007, 'recall': 0.9154926645232302, 'f1-score': 0.9219315460291992, 'support': 5676.0}, 'weighted avg': {'precision': 0.9362427594764959, 'recall': 0.9367512332628611, 'f1-score': 0.9362418626947815, 'support': 5676.0}}\n",
      "\n",
      "Model: SVM - Representation: Bag-of-Words\n",
      "Accuracy: 0.9464411557434813\n",
      "Classification Report:\n",
      "{'0': {'precision': 0.8811149032992036, 'recall': 0.9422141119221411, 'f1-score': 0.9106407995296883, 'support': 1644.0}, '1': {'precision': 0.9757529351710056, 'recall': 0.9481646825396826, 'f1-score': 0.961761006289308, 'support': 4032.0}, 'accuracy': 0.9464411557434813, 'macro avg': {'precision': 0.9284339192351045, 'recall': 0.9451893972309118, 'f1-score': 0.9362009029094982, 'support': 5676.0}, 'weighted avg': {'precision': 0.9483419195971432, 'recall': 0.9464411557434813, 'f1-score': 0.9469545193420187, 'support': 5676.0}}\n",
      "\n",
      "Model: SVM - Representation: TF-IDF\n",
      "Accuracy: 0.95630725863284\n",
      "Classification Report:\n",
      "{'0': {'precision': 0.9077102803738317, 'recall': 0.9452554744525548, 'f1-score': 0.9261025029797377, 'support': 1644.0}, '1': {'precision': 0.9772956609485368, 'recall': 0.9608134920634921, 'f1-score': 0.9689844922461232, 'support': 4032.0}, 'accuracy': 0.95630725863284, 'macro avg': {'precision': 0.9425029706611843, 'recall': 0.9530344832580234, 'f1-score': 0.9475434976129304, 'support': 5676.0}, 'weighted avg': {'precision': 0.9571409101266878, 'recall': 0.95630725863284, 'f1-score': 0.9565641274903202, 'support': 5676.0}}\n",
      "\n",
      "Model: SVM - Representation: Word2Vec\n",
      "Accuracy: 0.8994009866102889\n",
      "Classification Report:\n",
      "{'0': {'precision': 0.7755521314843349, 'recall': 0.9184914841849149, 'f1-score': 0.8409913673071567, 'support': 1644.0}, '1': {'precision': 0.9640654330919818, 'recall': 0.8916170634920635, 'f1-score': 0.9264270068290169, 'support': 4032.0}, 'accuracy': 0.8994009866102889, 'macro avg': {'precision': 0.8698087822881584, 'recall': 0.9050542738384892, 'f1-score': 0.8837091870680869, 'support': 5676.0}, 'weighted avg': {'precision': 0.9094643288208452, 'recall': 0.8994009866102889, 'f1-score': 0.9016813776228968, 'support': 5676.0}}\n",
      "\n",
      "Model: Random Forest - Representation: Bag-of-Words\n",
      "Accuracy: 0.9466173361522199\n",
      "Classification Report:\n",
      "{'0': {'precision': 0.9100917431192661, 'recall': 0.9051094890510949, 'f1-score': 0.9075937785910339, 'support': 1644.0}, '1': {'precision': 0.961395694135115, 'recall': 0.9635416666666666, 'f1-score': 0.9624674842066147, 'support': 4032.0}, 'accuracy': 0.9466173361522199, 'macro avg': {'precision': 0.9357437186271906, 'recall': 0.9343255778588808, 'f1-score': 0.9350306313988243, 'support': 5676.0}, 'weighted avg': {'precision': 0.946535987392681, 'recall': 0.9466173361522199, 'f1-score': 0.9465738316287402, 'support': 5676.0}}\n",
      "\n",
      "Model: Random Forest - Representation: TF-IDF\n",
      "Accuracy: 0.9452078928823114\n",
      "Classification Report:\n",
      "{'0': {'precision': 0.9022329511164756, 'recall': 0.9093673965936739, 'f1-score': 0.9057861254165405, 'support': 1644.0}, '1': {'precision': 0.9629261010201543, 'recall': 0.9598214285714286, 'f1-score': 0.9613712582287915, 'support': 4032.0}, 'accuracy': 0.9452078928823114, 'macro avg': {'precision': 0.932579526068315, 'recall': 0.9345944125825513, 'f1-score': 0.9335786918226661, 'support': 5676.0}, 'weighted avg': {'precision': 0.9453469011537611, 'recall': 0.9452078928823114, 'f1-score': 0.9452715474565327, 'support': 5676.0}}\n",
      "\n",
      "Model: Random Forest - Representation: Word2Vec\n",
      "Accuracy: 0.8731501057082452\n",
      "Classification Report:\n",
      "{'0': {'precision': 0.8442622950819673, 'recall': 0.6891727493917275, 'f1-score': 0.7588747488278633, 'support': 1644.0}, '1': {'precision': 0.882095062298108, 'recall': 0.9481646825396826, 'f1-score': 0.9139373655271338, 'support': 4032.0}, 'accuracy': 0.8731501057082452, 'macro avg': {'precision': 0.8631786786900376, 'recall': 0.818668715965705, 'f1-score': 0.8364060571774985, 'support': 5676.0}, 'weighted avg': {'precision': 0.8711371572059066, 'recall': 0.8731501057082452, 'f1-score': 0.8690249374345332, 'support': 5676.0}}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Class</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Bag-of-Words</td>\n",
       "      <td>0</td>\n",
       "      <td>0.952607</td>\n",
       "      <td>0.910938</td>\n",
       "      <td>0.927007</td>\n",
       "      <td>0.918903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Bag-of-Words</td>\n",
       "      <td>1</td>\n",
       "      <td>0.952607</td>\n",
       "      <td>0.970022</td>\n",
       "      <td>0.963046</td>\n",
       "      <td>0.966521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0</td>\n",
       "      <td>0.950669</td>\n",
       "      <td>0.887060</td>\n",
       "      <td>0.950730</td>\n",
       "      <td>0.917792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>1</td>\n",
       "      <td>0.950669</td>\n",
       "      <td>0.979305</td>\n",
       "      <td>0.950645</td>\n",
       "      <td>0.964762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>0</td>\n",
       "      <td>0.853770</td>\n",
       "      <td>0.694737</td>\n",
       "      <td>0.883212</td>\n",
       "      <td>0.777718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>1</td>\n",
       "      <td>0.853770</td>\n",
       "      <td>0.946458</td>\n",
       "      <td>0.841766</td>\n",
       "      <td>0.891048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Bag-of-Words</td>\n",
       "      <td>0</td>\n",
       "      <td>0.945913</td>\n",
       "      <td>0.898154</td>\n",
       "      <td>0.917275</td>\n",
       "      <td>0.907614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Bag-of-Words</td>\n",
       "      <td>1</td>\n",
       "      <td>0.945913</td>\n",
       "      <td>0.965974</td>\n",
       "      <td>0.957589</td>\n",
       "      <td>0.961764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0</td>\n",
       "      <td>0.936751</td>\n",
       "      <td>0.912123</td>\n",
       "      <td>0.864964</td>\n",
       "      <td>0.887918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>1</td>\n",
       "      <td>0.936751</td>\n",
       "      <td>0.946077</td>\n",
       "      <td>0.966022</td>\n",
       "      <td>0.955946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SVM</td>\n",
       "      <td>Bag-of-Words</td>\n",
       "      <td>0</td>\n",
       "      <td>0.946441</td>\n",
       "      <td>0.881115</td>\n",
       "      <td>0.942214</td>\n",
       "      <td>0.910641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SVM</td>\n",
       "      <td>Bag-of-Words</td>\n",
       "      <td>1</td>\n",
       "      <td>0.946441</td>\n",
       "      <td>0.975753</td>\n",
       "      <td>0.948165</td>\n",
       "      <td>0.961761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SVM</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0</td>\n",
       "      <td>0.956307</td>\n",
       "      <td>0.907710</td>\n",
       "      <td>0.945255</td>\n",
       "      <td>0.926103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SVM</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>1</td>\n",
       "      <td>0.956307</td>\n",
       "      <td>0.977296</td>\n",
       "      <td>0.960813</td>\n",
       "      <td>0.968984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SVM</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>0</td>\n",
       "      <td>0.899401</td>\n",
       "      <td>0.775552</td>\n",
       "      <td>0.918491</td>\n",
       "      <td>0.840991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SVM</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>1</td>\n",
       "      <td>0.899401</td>\n",
       "      <td>0.964065</td>\n",
       "      <td>0.891617</td>\n",
       "      <td>0.926427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Bag-of-Words</td>\n",
       "      <td>0</td>\n",
       "      <td>0.946617</td>\n",
       "      <td>0.910092</td>\n",
       "      <td>0.905109</td>\n",
       "      <td>0.907594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Bag-of-Words</td>\n",
       "      <td>1</td>\n",
       "      <td>0.946617</td>\n",
       "      <td>0.961396</td>\n",
       "      <td>0.963542</td>\n",
       "      <td>0.962467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0</td>\n",
       "      <td>0.945208</td>\n",
       "      <td>0.902233</td>\n",
       "      <td>0.909367</td>\n",
       "      <td>0.905786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>1</td>\n",
       "      <td>0.945208</td>\n",
       "      <td>0.962926</td>\n",
       "      <td>0.959821</td>\n",
       "      <td>0.961371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>0</td>\n",
       "      <td>0.873150</td>\n",
       "      <td>0.844262</td>\n",
       "      <td>0.689173</td>\n",
       "      <td>0.758875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>1</td>\n",
       "      <td>0.873150</td>\n",
       "      <td>0.882095</td>\n",
       "      <td>0.948165</td>\n",
       "      <td>0.913937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model Representation  Class  Accuracy  Precision    Recall  \\\n",
       "0   Logistic Regression   Bag-of-Words      0  0.952607   0.910938  0.927007   \n",
       "1   Logistic Regression   Bag-of-Words      1  0.952607   0.970022  0.963046   \n",
       "2   Logistic Regression         TF-IDF      0  0.950669   0.887060  0.950730   \n",
       "3   Logistic Regression         TF-IDF      1  0.950669   0.979305  0.950645   \n",
       "4   Logistic Regression       Word2Vec      0  0.853770   0.694737  0.883212   \n",
       "5   Logistic Regression       Word2Vec      1  0.853770   0.946458  0.841766   \n",
       "6           Naive Bayes   Bag-of-Words      0  0.945913   0.898154  0.917275   \n",
       "7           Naive Bayes   Bag-of-Words      1  0.945913   0.965974  0.957589   \n",
       "8           Naive Bayes         TF-IDF      0  0.936751   0.912123  0.864964   \n",
       "9           Naive Bayes         TF-IDF      1  0.936751   0.946077  0.966022   \n",
       "10                  SVM   Bag-of-Words      0  0.946441   0.881115  0.942214   \n",
       "11                  SVM   Bag-of-Words      1  0.946441   0.975753  0.948165   \n",
       "12                  SVM         TF-IDF      0  0.956307   0.907710  0.945255   \n",
       "13                  SVM         TF-IDF      1  0.956307   0.977296  0.960813   \n",
       "14                  SVM       Word2Vec      0  0.899401   0.775552  0.918491   \n",
       "15                  SVM       Word2Vec      1  0.899401   0.964065  0.891617   \n",
       "16        Random Forest   Bag-of-Words      0  0.946617   0.910092  0.905109   \n",
       "17        Random Forest   Bag-of-Words      1  0.946617   0.961396  0.963542   \n",
       "18        Random Forest         TF-IDF      0  0.945208   0.902233  0.909367   \n",
       "19        Random Forest         TF-IDF      1  0.945208   0.962926  0.959821   \n",
       "20        Random Forest       Word2Vec      0  0.873150   0.844262  0.689173   \n",
       "21        Random Forest       Word2Vec      1  0.873150   0.882095  0.948165   \n",
       "\n",
       "    F1-Score  \n",
       "0   0.918903  \n",
       "1   0.966521  \n",
       "2   0.917792  \n",
       "3   0.964762  \n",
       "4   0.777718  \n",
       "5   0.891048  \n",
       "6   0.907614  \n",
       "7   0.961764  \n",
       "8   0.887918  \n",
       "9   0.955946  \n",
       "10  0.910641  \n",
       "11  0.961761  \n",
       "12  0.926103  \n",
       "13  0.968984  \n",
       "14  0.840991  \n",
       "15  0.926427  \n",
       "16  0.907594  \n",
       "17  0.962467  \n",
       "18  0.905786  \n",
       "19  0.961371  \n",
       "20  0.758875  \n",
       "21  0.913937  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def train_and_evaluate(model, X_train, X_test, y_train, y_test, model_name, representation_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "    print(f\"Model: {model_name} - Representation: {representation_name}\\nAccuracy: {accuracy}\\nClassification Report:\\n{report_dict}\\n\")\n",
    "    \n",
    "    # Creating metric rows for each class\n",
    "    metrics_0 = {\n",
    "        'Model': model_name,\n",
    "        'Representation': representation_name,\n",
    "        'Class': 0,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': report_dict['0']['precision'],\n",
    "        'Recall': report_dict['0']['recall'],\n",
    "        'F1-Score': report_dict['0']['f1-score']\n",
    "    }\n",
    "\n",
    "    metrics_1 = {\n",
    "        'Model': model_name,\n",
    "        'Representation': representation_name,\n",
    "        'Class': 1,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': report_dict['1']['precision'],\n",
    "        'Recall': report_dict['1']['recall'],\n",
    "        'F1-Score': report_dict['1']['f1-score']\n",
    "    }\n",
    "\n",
    "    return metrics_0, metrics_1\n",
    "\n",
    "# List to store the results\n",
    "results = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    # Repeat for each representation like Bag-of-Words, TF-IDF, Word2Vec\n",
    "    # Example for Bag-of-Words\n",
    "    X_train, X_test, y_train, y_test = bow_representation(df_reviews)\n",
    "    result_0, result_1 = train_and_evaluate(model, X_train, X_test, y_train, y_test, model_name, \"Bag-of-Words\")\n",
    "    results.extend([result_0, result_1])\n",
    "\n",
    "    # TF-IDF\n",
    "    X_train, X_test, y_train, y_test = tfidf_representation(df_reviews)\n",
    "    result_0, result_1 = train_and_evaluate(model, X_train, X_test, y_train, y_test, model_name, \"TF-IDF\")\n",
    "    results.extend([result_0, result_1])\n",
    "\n",
    "    # Word2Vec (excluding Naive Bayes)\n",
    "    if model_name != 'Naive Bayes':\n",
    "        X_train, X_test, y_train, y_test = word2vec_representation(df_reviews, './data/cbow_s50.txt')\n",
    "        result_0, result_1 = train_and_evaluate(model, X_train, X_test, y_train, y_test, model_name, \"Word2Vec\")\n",
    "        results.extend([result_0, result_1])\n",
    "\n",
    "# Converting the list of results into a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Accuracy 0 (Negative</th>\n",
       "      <th>Accuracy 1 (Positive</th>\n",
       "      <th>F1-Score 0 (Negative</th>\n",
       "      <th>F1-Score 1 (Positive</th>\n",
       "      <th>Precision 0 (Negative</th>\n",
       "      <th>Precision 1 (Positive</th>\n",
       "      <th>Recall 0 (Negative</th>\n",
       "      <th>Recall 1 (Positive</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th>Representation</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Logistic Regression</th>\n",
       "      <th>Bag-of-Words</th>\n",
       "      <td>0.952607</td>\n",
       "      <td>0.952607</td>\n",
       "      <td>0.918903</td>\n",
       "      <td>0.966521</td>\n",
       "      <td>0.910938</td>\n",
       "      <td>0.970022</td>\n",
       "      <td>0.927007</td>\n",
       "      <td>0.963046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF</th>\n",
       "      <td>0.950669</td>\n",
       "      <td>0.950669</td>\n",
       "      <td>0.917792</td>\n",
       "      <td>0.964762</td>\n",
       "      <td>0.887060</td>\n",
       "      <td>0.979305</td>\n",
       "      <td>0.950730</td>\n",
       "      <td>0.950645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word2Vec</th>\n",
       "      <td>0.853770</td>\n",
       "      <td>0.853770</td>\n",
       "      <td>0.777718</td>\n",
       "      <td>0.891048</td>\n",
       "      <td>0.694737</td>\n",
       "      <td>0.946458</td>\n",
       "      <td>0.883212</td>\n",
       "      <td>0.841766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Naive Bayes</th>\n",
       "      <th>Bag-of-Words</th>\n",
       "      <td>0.945913</td>\n",
       "      <td>0.945913</td>\n",
       "      <td>0.907614</td>\n",
       "      <td>0.961764</td>\n",
       "      <td>0.898154</td>\n",
       "      <td>0.965974</td>\n",
       "      <td>0.917275</td>\n",
       "      <td>0.957589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF</th>\n",
       "      <td>0.936751</td>\n",
       "      <td>0.936751</td>\n",
       "      <td>0.887918</td>\n",
       "      <td>0.955946</td>\n",
       "      <td>0.912123</td>\n",
       "      <td>0.946077</td>\n",
       "      <td>0.864964</td>\n",
       "      <td>0.966022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Random Forest</th>\n",
       "      <th>Bag-of-Words</th>\n",
       "      <td>0.946617</td>\n",
       "      <td>0.946617</td>\n",
       "      <td>0.907594</td>\n",
       "      <td>0.962467</td>\n",
       "      <td>0.910092</td>\n",
       "      <td>0.961396</td>\n",
       "      <td>0.905109</td>\n",
       "      <td>0.963542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF</th>\n",
       "      <td>0.945208</td>\n",
       "      <td>0.945208</td>\n",
       "      <td>0.905786</td>\n",
       "      <td>0.961371</td>\n",
       "      <td>0.902233</td>\n",
       "      <td>0.962926</td>\n",
       "      <td>0.909367</td>\n",
       "      <td>0.959821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word2Vec</th>\n",
       "      <td>0.873150</td>\n",
       "      <td>0.873150</td>\n",
       "      <td>0.758875</td>\n",
       "      <td>0.913937</td>\n",
       "      <td>0.844262</td>\n",
       "      <td>0.882095</td>\n",
       "      <td>0.689173</td>\n",
       "      <td>0.948165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">SVM</th>\n",
       "      <th>Bag-of-Words</th>\n",
       "      <td>0.946441</td>\n",
       "      <td>0.946441</td>\n",
       "      <td>0.910641</td>\n",
       "      <td>0.961761</td>\n",
       "      <td>0.881115</td>\n",
       "      <td>0.975753</td>\n",
       "      <td>0.942214</td>\n",
       "      <td>0.948165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF</th>\n",
       "      <td>0.956307</td>\n",
       "      <td>0.956307</td>\n",
       "      <td>0.926103</td>\n",
       "      <td>0.968984</td>\n",
       "      <td>0.907710</td>\n",
       "      <td>0.977296</td>\n",
       "      <td>0.945255</td>\n",
       "      <td>0.960813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word2Vec</th>\n",
       "      <td>0.899401</td>\n",
       "      <td>0.899401</td>\n",
       "      <td>0.840991</td>\n",
       "      <td>0.926427</td>\n",
       "      <td>0.775552</td>\n",
       "      <td>0.964065</td>\n",
       "      <td>0.918491</td>\n",
       "      <td>0.891617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Accuracy 0 (Negative  \\\n",
       "Model               Representation                         \n",
       "Logistic Regression Bag-of-Words                0.952607   \n",
       "                    TF-IDF                      0.950669   \n",
       "                    Word2Vec                    0.853770   \n",
       "Naive Bayes         Bag-of-Words                0.945913   \n",
       "                    TF-IDF                      0.936751   \n",
       "Random Forest       Bag-of-Words                0.946617   \n",
       "                    TF-IDF                      0.945208   \n",
       "                    Word2Vec                    0.873150   \n",
       "SVM                 Bag-of-Words                0.946441   \n",
       "                    TF-IDF                      0.956307   \n",
       "                    Word2Vec                    0.899401   \n",
       "\n",
       "                                    Accuracy 1 (Positive  \\\n",
       "Model               Representation                         \n",
       "Logistic Regression Bag-of-Words                0.952607   \n",
       "                    TF-IDF                      0.950669   \n",
       "                    Word2Vec                    0.853770   \n",
       "Naive Bayes         Bag-of-Words                0.945913   \n",
       "                    TF-IDF                      0.936751   \n",
       "Random Forest       Bag-of-Words                0.946617   \n",
       "                    TF-IDF                      0.945208   \n",
       "                    Word2Vec                    0.873150   \n",
       "SVM                 Bag-of-Words                0.946441   \n",
       "                    TF-IDF                      0.956307   \n",
       "                    Word2Vec                    0.899401   \n",
       "\n",
       "                                    F1-Score 0 (Negative  \\\n",
       "Model               Representation                         \n",
       "Logistic Regression Bag-of-Words                0.918903   \n",
       "                    TF-IDF                      0.917792   \n",
       "                    Word2Vec                    0.777718   \n",
       "Naive Bayes         Bag-of-Words                0.907614   \n",
       "                    TF-IDF                      0.887918   \n",
       "Random Forest       Bag-of-Words                0.907594   \n",
       "                    TF-IDF                      0.905786   \n",
       "                    Word2Vec                    0.758875   \n",
       "SVM                 Bag-of-Words                0.910641   \n",
       "                    TF-IDF                      0.926103   \n",
       "                    Word2Vec                    0.840991   \n",
       "\n",
       "                                    F1-Score 1 (Positive  \\\n",
       "Model               Representation                         \n",
       "Logistic Regression Bag-of-Words                0.966521   \n",
       "                    TF-IDF                      0.964762   \n",
       "                    Word2Vec                    0.891048   \n",
       "Naive Bayes         Bag-of-Words                0.961764   \n",
       "                    TF-IDF                      0.955946   \n",
       "Random Forest       Bag-of-Words                0.962467   \n",
       "                    TF-IDF                      0.961371   \n",
       "                    Word2Vec                    0.913937   \n",
       "SVM                 Bag-of-Words                0.961761   \n",
       "                    TF-IDF                      0.968984   \n",
       "                    Word2Vec                    0.926427   \n",
       "\n",
       "                                    Precision 0 (Negative  \\\n",
       "Model               Representation                          \n",
       "Logistic Regression Bag-of-Words                 0.910938   \n",
       "                    TF-IDF                       0.887060   \n",
       "                    Word2Vec                     0.694737   \n",
       "Naive Bayes         Bag-of-Words                 0.898154   \n",
       "                    TF-IDF                       0.912123   \n",
       "Random Forest       Bag-of-Words                 0.910092   \n",
       "                    TF-IDF                       0.902233   \n",
       "                    Word2Vec                     0.844262   \n",
       "SVM                 Bag-of-Words                 0.881115   \n",
       "                    TF-IDF                       0.907710   \n",
       "                    Word2Vec                     0.775552   \n",
       "\n",
       "                                    Precision 1 (Positive  Recall 0 (Negative  \\\n",
       "Model               Representation                                              \n",
       "Logistic Regression Bag-of-Words                 0.970022            0.927007   \n",
       "                    TF-IDF                       0.979305            0.950730   \n",
       "                    Word2Vec                     0.946458            0.883212   \n",
       "Naive Bayes         Bag-of-Words                 0.965974            0.917275   \n",
       "                    TF-IDF                       0.946077            0.864964   \n",
       "Random Forest       Bag-of-Words                 0.961396            0.905109   \n",
       "                    TF-IDF                       0.962926            0.909367   \n",
       "                    Word2Vec                     0.882095            0.689173   \n",
       "SVM                 Bag-of-Words                 0.975753            0.942214   \n",
       "                    TF-IDF                       0.977296            0.945255   \n",
       "                    Word2Vec                     0.964065            0.918491   \n",
       "\n",
       "                                    Recall 1 (Positive  \n",
       "Model               Representation                      \n",
       "Logistic Regression Bag-of-Words              0.963046  \n",
       "                    TF-IDF                    0.950645  \n",
       "                    Word2Vec                  0.841766  \n",
       "Naive Bayes         Bag-of-Words              0.957589  \n",
       "                    TF-IDF                    0.966022  \n",
       "Random Forest       Bag-of-Words              0.963542  \n",
       "                    TF-IDF                    0.959821  \n",
       "                    Word2Vec                  0.948165  \n",
       "SVM                 Bag-of-Words              0.948165  \n",
       "                    TF-IDF                    0.960813  \n",
       "                    Word2Vec                  0.891617  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pivot_df = results_df.pivot_table(index=['Model', 'Representation'], columns='Class', values=['Accuracy', 'Precision', 'Recall', 'F1-Score'])\n",
    "\n",
    "# Renaming columns for clarity\n",
    "pivot_df.columns = [f'{col[0]} {col[1]} ({[\"Negative\",\"Positive\"][col[1]]}' for col in pivot_df.columns]\n",
    "\n",
    "display(pivot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
